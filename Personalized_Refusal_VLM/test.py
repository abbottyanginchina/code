import requests
from PIL import Image

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration

model_id = "/gpu02home/jmy5701/gpu/models/llava-1.5-7b-hf"

# ğŸš€ åŠ è½½æ¨¡å‹ï¼šå®Œå…¨ç”± accelerate è‡ªåŠ¨åˆ†é…åˆ° GPUï¼ˆä¸è¦å† .toï¼‰
model = LlavaForConditionalGeneration.from_pretrained(
    model_id, 
    torch_dtype=torch.float16,
    device_map="auto",          # æ­£ç¡®
    low_cpu_mem_usage=True,     # æ­£ç¡®
)

processor = AutoProcessor.from_pretrained(model_id)

conversation = [
    {
      "role": "user",
      "content": [
          {"type": "text", "text": "What are these?"},
          {"type": "image"},
        ],
    },
]

prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

image_file = "http://images.cocodataset.org/val2017/000000039769.jpg"
raw_image = Image.open(requests.get(image_file, stream=True).raw)

# ğŸŸ¢ æ­£ç¡®çš„è¾“å…¥å¤„ç†æ–¹å¼ï¼ˆä¸ä¼šçˆ†æ˜¾å­˜ï¼‰
processed = processor(
    images=raw_image,
    text=prompt,
    return_tensors='pt'
)

# ğŸŸ© é€ä¸ªæ¬åˆ° GPUï¼ˆH100 ä¸ä¼šç¢ç‰‡/ä¸ä¼š OOMï¼‰
inputs = {k: v.to("cuda", dtype=torch.float16, non_blocking=True)
          for k, v in processed.items()}

# ğŸš€ æ¨ç†
output = model.generate(**inputs, max_new_tokens=200, do_sample=False)

# ğŸ“ è§£ç 
print(processor.decode(output[0][2:], skip_special_tokens=True))