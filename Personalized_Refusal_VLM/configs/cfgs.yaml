training:
  batch_size: 8
  epochs: 200
  lr: 1e-4
pos: -1
model_name: "llava-1.5-7b-hf"
model_path: "/root/autodl-tmp"
seed: 42
max_new_tokens: 40
alpha_text: 1.8
num_train: 200
num_test: 200
  