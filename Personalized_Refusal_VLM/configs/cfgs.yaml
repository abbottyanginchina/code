training:
  batch_size: 8
  epochs: 200
  lr: 1e-4
pos: -1
model_name: "llava-1.5-7b-hf"
model_path: "../../models"
seed: 40
max_new_tokens: 40
alpha_text: 1.2
num_train: 200
num_test: 200
start_layer: 0
end_layer: 32

data:
  path: "./data"
  dataset_name: "MMMU"
  filter_data: False
  subject: "biology"

output_dir: "../results"

  